{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/driphics/GPT-2-model/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPUpwXB4AZGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3df5dbb4-417f-464d-aad5-052eafcc85f9"
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2.git"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 209, done.\u001b[K\n",
            "Receiving objects:   0% (1/209)   \rReceiving objects:   1% (3/209)   \rReceiving objects:   2% (5/209)   \rReceiving objects:   3% (7/209)   \rReceiving objects:   4% (9/209)   \rReceiving objects:   5% (11/209)   \rReceiving objects:   6% (13/209)   \rReceiving objects:   7% (15/209)   \rReceiving objects:   8% (17/209)   \rReceiving objects:   9% (19/209)   \rReceiving objects:  10% (21/209)   \rReceiving objects:  11% (23/209)   \rReceiving objects:  12% (26/209)   \rReceiving objects:  13% (28/209)   \rReceiving objects:  14% (30/209)   \rReceiving objects:  15% (32/209)   \rReceiving objects:  16% (34/209)   \rReceiving objects:  17% (36/209)   \rReceiving objects:  18% (38/209)   \rReceiving objects:  19% (40/209)   \rReceiving objects:  20% (42/209)   \rReceiving objects:  21% (44/209)   \rReceiving objects:  22% (46/209)   \rReceiving objects:  23% (49/209)   \rReceiving objects:  24% (51/209)   \rReceiving objects:  25% (53/209)   \rReceiving objects:  26% (55/209)   \rReceiving objects:  27% (57/209)   \rReceiving objects:  28% (59/209)   \rReceiving objects:  29% (61/209)   \rReceiving objects:  30% (63/209)   \rReceiving objects:  31% (65/209)   \rReceiving objects:  32% (67/209)   \rReceiving objects:  33% (69/209)   \rReceiving objects:  34% (72/209)   \rReceiving objects:  35% (74/209)   \rReceiving objects:  36% (76/209)   \rReceiving objects:  37% (78/209)   \rReceiving objects:  38% (80/209)   \rReceiving objects:  39% (82/209)   \rReceiving objects:  40% (84/209)   \rReceiving objects:  41% (86/209)   \rReceiving objects:  42% (88/209)   \rReceiving objects:  43% (90/209)   \rReceiving objects:  44% (92/209)   \rReceiving objects:  45% (95/209)   \rReceiving objects:  46% (97/209)   \rremote: Total 209 (delta 0), reused 0 (delta 0), pack-reused 209\u001b[K\n",
            "Receiving objects:  47% (99/209)   \rReceiving objects:  48% (101/209)   \rReceiving objects:  49% (103/209)   \rReceiving objects:  50% (105/209)   \rReceiving objects:  51% (107/209)   \rReceiving objects:  52% (109/209)   \rReceiving objects:  53% (111/209)   \rReceiving objects:  54% (113/209)   \rReceiving objects:  55% (115/209)   \rReceiving objects:  56% (118/209)   \rReceiving objects:  57% (120/209)   \rReceiving objects:  58% (122/209)   \rReceiving objects:  59% (124/209)   \rReceiving objects:  60% (126/209)   \rReceiving objects:  61% (128/209)   \rReceiving objects:  62% (130/209)   \rReceiving objects:  63% (132/209)   \rReceiving objects:  64% (134/209)   \rReceiving objects:  65% (136/209)   \rReceiving objects:  66% (138/209)   \rReceiving objects:  67% (141/209)   \rReceiving objects:  68% (143/209)   \rReceiving objects:  69% (145/209)   \rReceiving objects:  70% (147/209)   \rReceiving objects:  71% (149/209)   \rReceiving objects:  72% (151/209)   \rReceiving objects:  73% (153/209)   \rReceiving objects:  74% (155/209)   \rReceiving objects:  75% (157/209)   \rReceiving objects:  76% (159/209)   \rReceiving objects:  77% (161/209)   \rReceiving objects:  78% (164/209)   \rReceiving objects:  79% (166/209)   \rReceiving objects:  80% (168/209)   \rReceiving objects:  81% (170/209)   \rReceiving objects:  82% (172/209)   \rReceiving objects:  83% (174/209)   \rReceiving objects:  84% (176/209)   \rReceiving objects:  85% (178/209)   \rReceiving objects:  86% (180/209)   \rReceiving objects:  87% (182/209)   \rReceiving objects:  88% (184/209)   \rReceiving objects:  89% (187/209)   \rReceiving objects:  90% (189/209)   \rReceiving objects:  91% (191/209)   \rReceiving objects:  92% (193/209)   \rReceiving objects:  93% (195/209)   \rReceiving objects:  94% (197/209)   \rReceiving objects:  95% (199/209)   \rReceiving objects:  96% (201/209)   \rReceiving objects:  97% (203/209)   \rReceiving objects:  98% (205/209)   \rReceiving objects:  99% (207/209)   \rReceiving objects: 100% (209/209)   \rReceiving objects: 100% (209/209), 4.37 MiB | 19.38 MiB/s, done.\n",
            "Resolving deltas:   0% (0/109)   \rResolving deltas:   3% (4/109)   \rResolving deltas:   9% (10/109)   \rResolving deltas:  10% (11/109)   \rResolving deltas:  11% (12/109)   \rResolving deltas:  13% (15/109)   \rResolving deltas:  20% (22/109)   \rResolving deltas:  23% (26/109)   \rResolving deltas:  24% (27/109)   \rResolving deltas:  25% (28/109)   \rResolving deltas:  29% (32/109)   \rResolving deltas:  35% (39/109)   \rResolving deltas:  40% (44/109)   \rResolving deltas:  44% (49/109)   \rResolving deltas:  46% (51/109)   \rResolving deltas:  53% (58/109)   \rResolving deltas:  54% (59/109)   \rResolving deltas:  64% (70/109)   \rResolving deltas:  67% (74/109)   \rResolving deltas:  69% (76/109)   \rResolving deltas:  70% (77/109)   \rResolving deltas:  76% (83/109)   \rResolving deltas:  80% (88/109)   \rResolving deltas:  83% (91/109)   \rResolving deltas:  88% (97/109)   \rResolving deltas:  89% (98/109)   \rResolving deltas: 100% (109/109)   \rResolving deltas: 100% (109/109), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm5mnNpZAgPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"gpt-2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW9T5210AxwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "929e04cf-a287-4ac6-fa0c-d700cbf0295a"
      },
      "source": [
        "!python3 download_model.py 345M"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 730kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 58.2Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 721kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:23, 59.6Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 6.68Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 53.1Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 45.6Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD78c6flA1Ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MrP5K4SBBD2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "7baf9376-24b1-44cd-df99-e85cb50aff81"
      },
      "source": [
        "os.chdir('src')\n",
        "!pip install regex"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609236 sha256=9e3ca5b630cad5692392f7e383176b9a485b6958d1fab149ecc0f888311a098d\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "Successfully built regex\n",
            "Installing collected packages: regex\n",
            "Successfully installed regex-2019.8.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOSBDc2FBHch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import model, sample, encoder\n",
        "\n",
        "def interact_model(\n",
        "    model_name,\n",
        "    seed,\n",
        "    nsamples,\n",
        "    batch_size,\n",
        "    length,\n",
        "    temperature,\n",
        "    top_k,\n",
        "    models_dir\n",
        "):\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = encoder.get_encoder(model_name, models_dir)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        while True:\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "            while not raw_text:\n",
        "                print('Prompt should not be empty!')\n",
        "                raw_text = input(\"Model prompt >>> \")\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            generated = 0\n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    generated += 1\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "                    print(text)\n",
        "            print(\"=\" * 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLyeSKOWBTSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interact_model(\n",
        "    '345M',\n",
        "    None,\n",
        "    1,\n",
        "    1,\n",
        "    300,\n",
        "    1,\n",
        "    0,\n",
        "    '/content/gpt-2/models'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4TgaT15Ette",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}